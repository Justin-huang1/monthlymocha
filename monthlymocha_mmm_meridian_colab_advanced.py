# -*- coding: utf-8 -*-
"""MonthlyMocha_MMM_Meridian_Colab_Advanced.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tvz3yWJTPrFWqiIDnBbF2T5CO_MDYhm2

# MonthlyMocha MMM — Meridian Colab **Advanced**

This notebook extends the basic pipeline with:
- **Dead-channel filtering** + strict sanity checks
- **Custom priors** (per-channel ROI, optional group priors), optional **robust likelihood**
- **Richer seasonality** via Fourier (tunable K)
- **Rolling time-based CV**, LOO/WAIC (**if available**) + holdout
- **Model ensembling** (stacking by LOO, or holdout-weighted)
- **Risk-aware budget optimization** (optimize expected KPI AND penalize variance / CVaR across posterior draws)

> Set Colab runtime to **T4 GPU**. Make sure your CSV is uploaded or reachable.

## 0) Install Meridian & deps
"""

!pip -q install --upgrade "google-meridian[colab,and-cuda]" arviz
import platform, sys
print(platform.python_version(), platform.platform())

import meridian, platform
print("Meridian version:", meridian.__version__)
print("Python:", platform.python_version())

try:
    import meridian, pandas as pd, numpy as np, platform
except Exception:
    !pip -q install --upgrade "google-meridian[colab,and-cuda]" arviz
    import meridian, pandas as pd, numpy as np, platform

print("Meridian:", getattr(meridian, "__version__", "?"), "| Python:", platform.python_version())

CSV_CANDIDATES = [
    "MMM Takehome Dataset (1).csv",
]
df = None
for p in CSV_CANDIDATES:
    try:
        df = pd.read_csv(p)
        CSV_PATH = p
        break
    except Exception:
        pass
assert df is not None, "Upload your CSV to Colab or set CSV_PATH accordingly."

TARGET = "subscriptions"
assert "date" in df.columns and TARGET in df.columns, "CSV must have `date` and `subscriptions`."


df = df.copy()
df["date"] = pd.to_datetime(df["date"])
df = df.sort_values("date").reset_index(drop=True)
df = df.rename(columns={"date": "time"})   # Meridian requires `time`

spend_cols = sorted([c for c in df.columns if c.endswith("_spend")])
spend_cols = [c for c in spend_cols if np.isfinite(df[c]).all()]
spend_cols = [c for c in spend_cols if df[c].abs().sum() > 0]
assert spend_cols, "No usable *_spend columns found."

df[spend_cols] = df[spend_cols].fillna(0.0).astype(float)
channels = [c.replace("_spend", "") for c in spend_cols]
print(f"Using {len(channels)} media channels:", channels)


from meridian.data import data_frame_input_data_builder as data_builder

builder = data_builder.DataFrameInputDataBuilder(
    kpi_type="non_revenue",
    default_kpi_column=TARGET,
)
builder = builder.with_kpi(df)

# Spends-only: use spends for BOTH media_cols and media_spend_cols
builder = builder.with_media(
    df,
    media_cols=spend_cols,
    media_spend_cols=spend_cols,
    media_channels=channels,
)
data = builder.build()

#  Model Spec & Fit
from meridian.model import spec, prior_distribution
from meridian.model import model as model_mod

pri = prior_distribution.PriorDistribution()        # keep priors weak/simple
model_spec = spec.ModelSpec(prior=pri)              # defaults are fine for first shot

mmm = model_mod.Meridian(input_data=data, model_spec=model_spec)
mmm.sample_prior(300)
mmm.sample_posterior(n_chains=2, n_adapt=300, n_burnin=300, n_keep=600)
print("Posterior sampling complete. Groups:", list(mmm.inference_data.groups()))


from meridian.analysis import summarizer
sumr = summarizer.Summarizer(mmm)

summary_df = None
for name in [
    "get_media_summary", "media_summary",
    "get_media_effects", "media_effects_summary",
    "get_channel_summary"
]:
    if hasattr(sumr, name):
        try:
            tbl = getattr(sumr, name)()
            summary_df = tbl.to_pandas() if hasattr(tbl, "to_pandas") else (tbl if isinstance(tbl, pd.DataFrame) else None)
            if summary_df is not None:
                break
        except Exception as e:
            print(f"[info] Summarizer.{name}() not usable: {e}")

def _nice_top(df, k=12):
    if df is None: return None
    # choose a good sort column if available
    for c in ["incremental_mean","incremental","mean_incremental","effect_mean","lift_mean","roi","mroi"]:
        if c in df.columns:
            return df.sort_values(c, ascending=False).head(k)
    return df.head(k)

top_summary = _nice_top(summary_df)
if top_summary is not None:
    print("\n=== Channel Summary (top 12) ===")
    display(top_summary)
else:
    print("\n[info] No built-in summary table on this wheel. Proceeding to optimizer view.")

#  +5% Budget Optimization
from meridian.analysis import optimizer

bo = optimizer.BudgetOptimizer(mmm)
hist_total_budget = float(df[spend_cols].sum().sum())
plus5_budget = hist_total_budget * 1.05

opt_results = bo.optimize(
    budget=plus5_budget,
    use_kpi=True,                    # critical for non-revenue KPI
    spend_constraint_lower=0.30,     # +/-30% channel deviations around baseline
    spend_constraint_upper=0.30,
)

#  Turn optimizer outputs into a clean per-channel table
def _to_df(xr_like):
    """Robustly convert opt result xarray to DataFrame with a detected 'channel' col."""
    d = xr_like.to_dataframe().reset_index()
    # find the channel column robustly
    candidate_cols = []
    # Add index names too (in case they're still present)
    idx_names = list(getattr(xr_like, "indexes", {}))
    if idx_names:
        candidate_cols.extend(idx_names)
    candidate_cols.extend(list(d.columns))
    candidate_cols = list(dict.fromkeys(candidate_cols))  # unique, keep order

    ch_col = None
    for c in candidate_cols:
        if c in d.columns:
            vals = d[c].astype(str)
            inter = set(vals.unique()).intersection(set(channels))
            if len(inter) >= max(1, int(0.3 * len(channels))):  # overlap heuristic
                ch_col = c
                break
    if ch_col is None:
        # last resort: look for string columns with many distinct values
        obj_cols = [c for c in d.columns if d[c].dtype == "O"]
        for c in obj_cols:
            vals = d[c].astype(str)
            inter = set(vals.unique()).intersection(set(channels))
            if len(inter) > 0:
                ch_col = c
                break
    if ch_col is None:
        raise KeyError("Could not detect channel column in optimizer output.")
    d = d.rename(columns={ch_col: "channel"})
    return d

opt_df  = _to_df(opt_results.optimized_data)
base_df = _to_df(opt_results.nonoptimized_data)

# keep useful numeric cols if present
keep = ["spend","incremental_outcome","roi","mroi","percentage_of_spend"]
opt_keep  = ["channel"] + [c for c in keep if c in opt_df.columns]
base_keep = ["channel"] + [c for c in keep if c in base_df.columns]

opt_view  = opt_df[opt_keep].copy()
base_view = base_df[base_keep].copy()

opt_view  = opt_view.rename(columns={c: f"opt_{c}"  for c in keep if c in opt_view.columns})
base_view = base_view.rename(columns={c: f"base_{c}" for c in keep if c in base_view.columns})

alloc_compare = base_view.merge(opt_view, on="channel", how="outer")
if "base_spend" in alloc_compare.columns and "opt_spend" in alloc_compare.columns:
    alloc_compare["delta_spend"] = alloc_compare["opt_spend"] - alloc_compare["base_spend"]
if "base_incremental_outcome" in alloc_compare.columns and "opt_incremental_outcome" in alloc_compare.columns:
    alloc_compare["delta_incremental"] = alloc_compare["opt_incremental_outcome"] - alloc_compare["base_incremental_outcome"]

print("\n=== Optimized vs Baseline (top 12 by delta_incremental if available) ===")
if "delta_incremental" in alloc_compare.columns:
    display(alloc_compare.sort_values("delta_incremental", ascending=False).head(12))
else:
    display(alloc_compare.head(12))

print("\nAll done ✅  (You can also call `opt_results.plot_*` helpers for visuals.)")

#  A) Add trend + seasonality controls

import numpy as np, pandas as pd
from meridian.data import data_frame_input_data_builder as data_builder
from meridian.model import spec, prior_distribution
from meridian.model import model as model_mod
from meridian.analysis import optimizer
from tensorflow_probability import distributions as tfd
from meridian import constants

# assumes df, spend_cols, channels, TARGET already exist from the base cell
df_ctrl = df.copy()

# simple time controls
df_ctrl["t"] = np.arange(len(df_ctrl))
df_ctrl["month_sin"] = np.sin(2*np.pi*df_ctrl["time"].dt.month/12)
df_ctrl["month_cos"] = np.cos(2*np.pi*df_ctrl["time"].dt.month/12)
control_cols = ["t","month_sin","month_cos"]

builder_c = data_builder.DataFrameInputDataBuilder(
    kpi_type="non_revenue",
    default_kpi_column=TARGET,
)
builder_c = builder_c.with_kpi(df_ctrl)
builder_c = builder_c.with_media(
    df_ctrl,
    media_cols=spend_cols,
    media_spend_cols=spend_cols,
    media_channels=channels,
)
builder_c = builder_c.with_controls(df_ctrl, control_cols=control_cols)
data_with_controls = builder_c.build()

# FAST priors/spec: fix adstock & slope to avoid extra sampling; modest EC prior
pri_fast = prior_distribution.PriorDistribution(
    alpha_m=tfd.Deterministic(0.60, name=constants.ALPHA_M),           # fixed geometric carryover
    slope_m=tfd.Deterministic(1.00, name=constants.SLOPE_M),           # fixed Hill slope
    ec_m   =tfd.TruncatedNormal(loc=1.0, scale=0.6, low=0.1, high=10.0, name=constants.EC_M),
)

spec_fast = spec.ModelSpec(
    prior=pri_fast,
    adstock_decay_spec="geometric",
    max_lag=8,
    enable_aks=False,     # we added explicit controls; keep model light
)

mmm_fast = model_mod.Meridian(input_data=data_with_controls, model_spec=spec_fast)
mmm_fast.sample_prior(100)

mmm_fast.sample_posterior(n_chains=1, n_adapt=200, n_burnin=200, n_keep=300)
print("FAST model done.")

# quick optimization in KPI mode (+5% budget)
bo_fast = optimizer.BudgetOptimizer(mmm_fast)
hist_total_budget = float(df[spend_cols].sum().sum())
opt_fast = bo_fast.optimize(
    budget=hist_total_budget*1.05,
    use_kpi=True,
    spend_constraint_lower=0.20,
    spend_constraint_upper=0.20,
)

# make table
def _opt_table(opt_results, channels):
    d1 = opt_results.optimized_data.to_dataframe().reset_index()
    d0 = opt_results.nonoptimized_data.to_dataframe().reset_index()
    # detect channel column
    ch_col = None
    for c in d1.columns:
        if set(d1[c].astype(str)) & set(channels):
            ch_col = c; break
    if ch_col is None:
        ch_candidates = [c for c in d1.columns if d1[c].dtype == 'O']
        ch_col = ch_candidates[0] if ch_candidates else d1.columns[0]
    d1 = d1.rename(columns={ch_col:"channel"})
    d0 = d0.rename(columns={ch_col:"channel"})
    keep = ["spend","incremental_outcome","roi","mroi","percentage_of_spend"]
    v1 = d1[["channel"]+[c for c in keep if c in d1.columns]].rename(columns={c:f"opt_{c}" for c in keep if c in d1.columns})
    v0 = d0[["channel"]+[c for c in keep if c in d0.columns]].rename(columns={c:f"base_{c}" for c in keep if c in d0.columns})
    out = v0.merge(v1, on="channel", how="outer")
    if {"base_spend","opt_spend"}.issubset(out.columns):
        out["delta_spend"] = out["opt_spend"] - out["base_spend"]
    if {"base_incremental_outcome","opt_incremental_outcome"}.issubset(out.columns):
        out["delta_incremental"] = out["opt_incremental_outcome"] - out["base_incremental_outcome"]
    return out

alloc_fast = _opt_table(opt_fast, channels)
display(alloc_fast.sort_values("delta_incremental", ascending=False).head(12))

#  Top-coverage grouping + fixed nonlinears

import numpy as np, pandas as pd, tensorflow as tf
from meridian.data import data_frame_input_data_builder as data_builder
from meridian.model import spec, prior_distribution
from meridian.model import model as model_mod
from meridian.analysis import optimizer
from tensorflow_probability import distributions as tfd
from meridian import constants


tf.keras.backend.set_floatx('float32')  # prefer float32 to reduce compute
np.random.seed(7)



# 1) Keep only channels that cover ~90% of total spend (auto K), bucket rest into 'other'
totals = df[spend_cols].sum().sort_values(ascending=False)
cum_share = (totals / totals.sum()).cumsum()
keep_list = list(totals.index[cum_share <= 0.90])  # channels up to 90%
# ensure at least 6 survive even if very concentrated
if len(keep_list) < 6:
    keep_list = list(totals.index[:6])

small_list = [c for c in spend_cols if c not in keep_list]

df_grp = df.copy()
if small_list:
    df_grp["other_spend"] = df_grp[small_list].sum(axis=1).astype('float32')
    media_cols = keep_list + ["other_spend"]
else:
    media_cols = keep_list

# enforce float32 for all media cols
df_grp[media_cols] = df_grp[media_cols].astype('float32')

channels2 = [c.replace("_spend","") for c in media_cols]
print(f"Kept {len(channels2)} channels covering ~90% spend:", channels2)


builder_g = data_builder.DataFrameInputDataBuilder(
    kpi_type="non_revenue",
    default_kpi_column=TARGET,
)
builder_g = builder_g.with_kpi(df_grp)
builder_g = builder_g.with_media(
    df_grp,
    media_cols=media_cols,
    media_spend_cols=media_cols,
    media_channels=channels2,
)
data_grp = builder_g.build()

# 3) Ultra-lean priors/spec:
#    - alpha_m = 0.55 (geometric carryover)
#    - slope_m = 1.0  (Hill slope)
#    - ec_m    = 1.0  (half-saturation point)
pri_ultra = prior_distribution.PriorDistribution(
    alpha_m=tfd.Deterministic(0.55, name=constants.ALPHA_M),
    slope_m=tfd.Deterministic(1.00, name=constants.SLOPE_M),
    ec_m   =tfd.Deterministic(1.00, name=constants.EC_M),
)

spec_ultra = spec.ModelSpec(
    prior=pri_ultra,
    adstock_decay_spec="geometric",
    max_lag=4,            # shorter lag window → faster
    enable_aks=False,     # keep graph minimal (use your base fit/optimizer for diagnostics)
)

# 4) Fit — tiny draws, 1 chain
mmm_grp = model_mod.Meridian(input_data=data_grp, model_spec=spec_ultra)
mmm_grp.sample_prior(60)
mmm_grp.sample_posterior(n_chains=1, n_adapt=120, n_burnin=100, n_keep=160)
print("Grouped fast model complete.")

# 5) Optimize for budget
bo_grp = optimizer.BudgetOptimizer(mmm_grp)
hist_total_budget_grp = float(df_grp[media_cols].sum().sum())
opt_grp = bo_grp.optimize(
    budget=hist_total_budget_grp * 1.05,
    use_kpi=True,
    spend_constraint_lower=0.15,
    spend_constraint_upper=0.15,
)

# 6) Table with results
def _opt_table(opt_results, channels):
    d1 = opt_results.optimized_data.to_dataframe().reset_index()
    d0 = opt_results.nonoptimized_data.to_dataframe().reset_index()
    # detect channel column robustly
    ch_col = None
    for c in d1.columns:
        if set(d1[c].astype(str)) & set(channels):
            ch_col = c; break
    if ch_col is None:
        str_cols = [c for c in d1.columns if d1[c].dtype == 'O']
        ch_col = str_cols[0] if str_cols else d1.columns[0]
    d1 = d1.rename(columns={ch_col:"channel"})
    d0 = d0.rename(columns={ch_col:"channel"})
    keep = ["spend","incremental_outcome","roi","mroi","percentage_of_spend"]
    v1 = d1[["channel"]+[c for c in keep if c in d1.columns]].rename(columns={c:f"opt_{c}" for c in keep if c in d1.columns})
    v0 = d0[["channel"]+[c for c in keep if c in d0.columns]].rename(columns={c:f"base_{c}" for c in keep if c in d0.columns})
    out = v0.merge(v1, on="channel", how="outer")
    if {"base_spend","opt_spend"}.issubset(out.columns):
        out["delta_spend"] = out["opt_spend"] - out["base_spend"]
    if {"base_incremental_outcome","opt_incremental_outcome"}.issubset(out.columns):
        out["delta_incremental"] = out["opt_incremental_outcome"] - out["base_incremental_outcome"]
    return out

alloc_grp = _opt_table(opt_grp, channels2)
display(alloc_grp.sort_values("delta_incremental", ascending=False).head(12))

# --- Part C: Efficiency table from baseline (marginal-ish ranking near current spend) ---

import numpy as np, pandas as pd
import math

# 1) Pick whichever optimizer results you already have
use_opt = None
for cand in ["opt_fast", "opt_grp", "opt_results"]:
    if cand in globals():
        use_opt = globals()[cand]
        print(f"[info] Using optimizer results from `{cand}`")
        break
assert use_opt is not None, "Run an optimizer cell first (e.g., base, Part A, or Part B)."

# 2) Pull the NON-optimized (baseline) xarray -> DataFrame
base_df = use_opt.nonoptimized_data.to_dataframe().reset_index()

# 3) Detect the channel column robustly
def _detect_channel_col(df, known_channels):
    # best: exact overlap with known channel names
    for c in df.columns:
        vals = df[c].astype(str)
        if len(set(vals) & set(known_channels)) >= max(1, int(0.3*len(known_channels))):
            return c
    # fallback: any object column having any overlap
    for c in df.columns:
        if df[c].dtype == "O":
            vals = df[c].astype(str)
            if len(set(vals) & set(known_channels)) > 0:
                return c
    # last resort: a column literally called 'channel' if present
    if "channel" in df.columns:
        return "channel"
    # give up: pick the first object column
    obj_cols = [c for c in df.columns if df[c].dtype == "O"]
    return obj_cols[0] if obj_cols else df.columns[0]

ch_col = _detect_channel_col(base_df, channels)
base_df = base_df.rename(columns={ch_col: "channel"})

# 4) Figure out canonical metric column names present in your wheel
#    We’ll prefer 'incremental_outcome' and 'spend' if available.
spend_col = "spend" if "spend" in base_df.columns else None
incr_col_candidates = ["incremental_outcome", "incremental", "expected_incremental_outcome"]
incr_col = next((c for c in incr_col_candidates if c in base_df.columns), None)

# Minimal sanity
if spend_col is None or incr_col is None:
    print("[warn] Couldn’t find expected metric columns; showing raw baseline frame head().")
    display(base_df.head(20))
else:
    # 5) Aggregate across time/any extra dims to per-channel totals/means
    agg_spec = {spend_col: "sum", incr_col: "sum"}
    if "roi" in base_df.columns:  agg_spec["roi"]  = "mean"
    if "mroi" in base_df.columns: agg_spec["mroi"] = "mean"

    eff = (base_df
           .groupby("channel", as_index=False)
           .agg(agg_spec)
           .rename(columns={spend_col: "base_spend", incr_col: "base_incremental"}))

    # 6) Efficiency score (incremental per $ at baseline)
    eff["efficiency"] = eff["base_incremental"] / eff["base_spend"].replace(0, np.nan)

    # Optional: a quick z-score to spot outliers
    m, s = eff["efficiency"].mean(skipna=True), eff["efficiency"].std(skipna=True)
    eff["efficiency_z"] = (eff["efficiency"] - m) / (s if (s and not math.isclose(s,0.0)) else np.nan)

    # 7) Rank and show top 12
    eff_ranked = eff.sort_values("efficiency", ascending=False)
    print("=== Baseline efficiency (incremental per $) — top 12 ===")
    display(eff_ranked.head(12))

    # 8) (Optional) small bar chart — uncomment if you want a visual
    import matplotlib.pyplot as plt
    topk = eff_ranked.head(10)
    plt.figure(figsize=(8,4))
    plt.bar(topk["channel"], topk["efficiency"])
    plt.xticks(rotation=45, ha="right")
    plt.title("Baseline Efficiency (incremental per $)")
    plt.tight_layout()
    plt.show()

# --- Part D: Tiny +2% budget what-if (fast, KPI-mode) ---

import numpy as np, pandas as pd
from meridian.analysis import optimizer

# 1) Pick the model to use (prefer the faster/most recent you've fit)
model_name = None
for cand in ["mmm_fast", "mmm_grp", "mmm"]:
    if cand in globals():
        model_name = cand
        m_use = globals()[cand]
        print(f"[info] Using model: {cand}")
        break
assert model_name is not None, "No Meridian model found. Run the base cell (or Part A/B) first."

# 2) Compute historical total budget from the right DataFrame/columns
if model_name == "mmm_grp" and "df_grp" in globals():
    # grouped case (Part B)
    media_cols_ref = globals().get("media_cols", None)
    if media_cols_ref is None:
        # fallback: infer from channels2 if present
        media_cols_ref = [c for c in df_grp.columns if c.endswith("_spend") or c == "other_spend"]
    hist_total_budget = float(df_grp[media_cols_ref].sum().sum())
else:
    # base / Part A case
    hist_total_budget = float(df[spend_cols].sum().sum())

# 3) +2% total budget, tight per-channel deviation (±10%)
bo = optimizer.BudgetOptimizer(m_use)
opt_2pct = bo.optimize(
    budget=hist_total_budget * 1.02,
    use_kpi=True,
    spend_constraint_lower=0.10,
    spend_constraint_upper=0.10,
)

# 4) Helper to tidy optimizer outputs (reuse if already defined)
def _opt_table(opt_results, channels_guess=None):
    d1 = opt_results.optimized_data.to_dataframe().reset_index()
    d0 = opt_results.nonoptimized_data.to_dataframe().reset_index()

    # detect channel column robustly
    ch_col = None
    # try known channel lists if provided in the session
    known_sets = []
    if channels_guess is not None: known_sets.append(set(channels_guess))
    if "channels2" in globals(): known_sets.append(set(globals()["channels2"]))
    if "channels" in globals():  known_sets.append(set(globals()["channels"]))
    known_sets.append(set([str(x) for x in d1.select_dtypes("object").stack().unique()]))

    for c in d1.columns:
        vals = set(d1[c].astype(str).unique())
        if any(vals & ks for ks in known_sets if ks):
            ch_col = c; break
    if ch_col is None:
        obj_cols = [c for c in d1.columns if d1[c].dtype == "O"]
        ch_col = obj_cols[0] if obj_cols else d1.columns[0]

    d1 = d1.rename(columns={ch_col: "channel"})
    d0 = d0.rename(columns={ch_col: "channel"})

    keep = ["spend","incremental_outcome","roi","mroi","percentage_of_spend"]
    v1 = d1[["channel"]+[c for c in keep if c in d1.columns]].rename(columns={c: f"opt_{c}" for c in keep if c in d1.columns})
    v0 = d0[["channel"]+[c for c in keep if c in d0.columns]].rename(columns={c: f"base_{c}" for c in keep if c in d0.columns})
    out = v0.merge(v1, on="channel", how="outer")

    if {"base_spend","opt_spend"}.issubset(out.columns):
        out["delta_spend"] = out["opt_spend"] - out["base_spend"]
    if {"base_incremental_outcome","opt_incremental_outcome"}.issubset(out.columns):
        out["delta_incremental"] = out["opt_incremental_outcome"] - out["base_incremental_outcome"]
    return out

# 5) Try to pass a reasonable channel list for nicer labeling
channels_guess = None
if model_name == "mmm_grp" and "channels2" in globals():
    channels_guess = globals()["channels2"]
elif "channels" in globals():
    channels_guess = globals()["channels"]

tab_2pct = _opt_table(opt_2pct, channels_guess)
print("=== +2% budget what-if (ranked by delta_incremental if available) ===")
if "delta_incremental" in tab_2pct.columns:
    display(tab_2pct.sort_values("delta_incremental", ascending=False).head(12))
else:
    display(tab_2pct.head(12))

# --- Part E: Meridian diagnostics + quick response curves ---

import numpy as np, pandas as pd
from meridian.analysis import visualizer, summarizer

# 1) Core diagnostics (try/except to survive wheel diffs)
try:
    _ = visualizer.ModelDiagnostics(mmm).plot_rhat_boxplot()
except Exception as e:
    print("[info] Rhat boxplot not available on this wheel:", e)

try:
    _ = visualizer.ModelFit(mmm).plot_model_fit()
except Exception as e:
    print("[info] Model fit plot not available on this wheel:", e)

# 2) Channel summary table (works if your wheel exposes any of these)
sumr = summarizer.Summarizer(mmm)
summary_df = None
for name in ["get_media_summary", "media_summary", "get_media_effects", "media_effects_summary", "get_channel_summary"]:
    if hasattr(sumr, name):
        try:
            tbl = getattr(sumr, name)()
            summary_df = tbl.to_pandas() if hasattr(tbl, "to_pandas") else (tbl if isinstance(tbl, pd.DataFrame) else None)
            if summary_df is not None:
                break
        except Exception:
            pass

if summary_df is not None:
    from IPython.display import display
    # sort by an intuitive column if present
    for c in ["incremental_mean","incremental","mean_incremental","roi","mroi"]:
        if c in summary_df.columns:
            display(summary_df.sort_values(c, ascending=False).head(12))
            break
else:
    print("[info] No media summary table found; using optimizer table instead if available.")
    try:
        display(alloc_compare.sort_values("delta_incremental", ascending=False).head(12))
    except Exception:
        print("[info] Run the earlier optimization cell first to get alloc_compare.")

# 3) “Local response” curves (quick marginal what-if for top channels)
#    For the top 3 channels by baseline spend, nudge ±25% and reopt within tight bounds
from meridian.analysis import optimizer
import matplotlib.pyplot as plt

# pick a model to use (prefer the one with controls if present)
m_use = mmm_fast if 'mmm_fast' in globals() else (mmm_grp if 'mmm_grp' in globals() else mmm)

# choose the reference DataFrame for historical baseline
if m_use is mmm_grp and 'df_grp' in globals():
    ref_df = df_grp.copy()
    media_cols_ref = [c for c in ref_df.columns if c.endswith("_spend") or c == "other_spend"]
    ch_names = [c.replace("_spend","") for c in media_cols_ref]
else:
    ref_df = df.copy()
    media_cols_ref = spend_cols
    ch_names = channels

base_spend = ref_df[media_cols_ref].sum()
top3_cols = list(base_spend.sort_values(ascending=False).head(3).index)
top3 = [c.replace("_spend","") for c in top3_cols]
print("Plotting local response for:", top3)

bo = optimizer.BudgetOptimizer(m_use)
total_budget = float(ref_df[media_cols_ref].sum().sum())

def one_channel_curve(col, steps=7, width=0.25):
    # vary this channel's budget ±width around baseline; keep others ±5%
    grid = np.linspace(1.0 - width, 1.0 + width, steps)
    rows = []
    for g in grid:
        # requested total budget equals original total (so optimizer can move within constraints)
        # per-channel constraints:
        lower = []
        upper = []
        # build constraints aligned to channel order in the model (ch_names)
        for c in ch_names:
            if c == col.replace("_spend",""):
                lower.append(max(0.0, g-0.001))  # ~fix to g
                upper.append(max(0.0, g-0.001))
            else:
                lower.append(0.05)  # allow small wiggle
                upper.append(0.05)
        try:
            res = bo.optimize(
                budget=total_budget,
                use_kpi=True,
                spend_constraint_lower=lower,
                spend_constraint_upper=upper,
            )
            # pull this channel's spend & incremental outcome after optimize
            odf = res.optimized_data.to_dataframe().reset_index()
            # detect channel column
            ch_col = None
            for k in odf.columns:
                vals = set(odf[k].astype(str))
                if len(vals & set(ch_names)) > 0: ch_col = k; break
            if ch_col is None:
                ch_col = [k for k in odf.columns if odf[k].dtype == 'O'][0]
            odf = odf.rename(columns={ch_col:"channel"})
            row = {
                "multiplier": g,
                "total_incremental": odf.get("incremental_outcome", pd.Series([np.nan])).sum(),
                "channel": col.replace("_spend","")
            }
            # optional: grab the focal channel's spend
            me = odf[odf["channel"] == col.replace("_spend","")]
            if not me.empty and "spend" in me.columns:
                row["spend"] = float(me["spend"].iloc[0])
            rows.append(row)
        except Exception as e:
            rows.append({"multiplier": g, "total_incremental": np.nan, "channel": col.replace("_spend","")})
    return pd.DataFrame(rows)

# build curves & plot
curves = []
for col in top3_cols:
    curves.append(one_channel_curve(col))
curves = pd.concat(curves, ignore_index=True)

fig, ax = plt.subplots(figsize=(7,4))
for ch in curves["channel"].unique():
    sub = curves[curves["channel"] == ch].sort_values("multiplier")
    ax.plot(sub["multiplier"], sub["total_incremental"], marker='o', label=ch)
ax.set_xlabel("Spend multiplier for channel (±25% around baseline)")
ax.set_ylabel("Optimized total incremental KPI")
ax.set_title("Local response (marginal what-if)")
ax.legend()
plt.tight_layout()
plt.show()

# --- Bayesian Linear Mixed MMM (PyMC, STA721-style) — aligned, <1000 draws, clean PPC ---

import numpy as np, pandas as pd
import pymc as pm
import arviz as az

# 0) Helpers
def geometric_adstock(x, alpha=0.6, max_lag=8):
    x = np.asarray(x, dtype=float)
    T = len(x)
    out = np.zeros(T, dtype=float)
    w = np.array([alpha**k for k in range(max_lag+1)], dtype=float)
    for t in range(T):
        k = min(max_lag, t)
        out[t] = (x[t-k:t+1][::-1] * w[:k+1]).sum()
    return out

def zscore_cols(M):
    M = np.asarray(M, dtype=float)
    mu = M.mean(axis=0, keepdims=True)
    sd = M.std(axis=0, keepdims=True)
    sd = np.where(sd == 0, 1.0, sd)
    return (M - mu) / sd, mu.ravel(), sd.ravel()

# 1) Build design from whatever active DataFrame/columns you’re using now.
# Prefer grouped Part B if present; otherwise fall back to base data.
if "df_grp" in globals() and "media_cols" in globals():
    df_use = df_grp
    cols_use = media_cols[:]  # includes 'other_spend' if created
else:
    df_use = df
    cols_use = spend_cols[:]

# Channel names aligned to columns in use
ch_names = [c.replace("_spend","") for c in cols_use]

# Adstocked (original units) and standardized versions
X_raw = np.column_stack([geometric_adstock(df_use[c].values, alpha=0.6, max_lag=8) for c in cols_use])
X, X_mu, X_sd = zscore_cols(X_raw)

y = df_use["subscriptions"].values.astype(float)
y_c = y - y.mean()

# Controls (trend + seasonality) on the same df_use, standardized
t = np.arange(len(df_use))
C_raw = np.column_stack([
    (t - t.mean())/t.std(),
    np.sin(2*np.pi*df_use["time"].dt.month.values/12.0),
    np.cos(2*np.pi*df_use["time"].dt.month.values/12.0),
])
C, C_mu, C_sd = zscore_cols(C_raw)
ctrl_names = ["trend","s_sin","s_cos"]

T, J = X.shape
assert J == len(ch_names), f"Design width J={J} but ch_names={len(ch_names)}."

# 2) Non-centered hierarchical mixed model
with pm.Model() as m_lmm:
    mu_theta   = pm.Normal("mu_theta", 0.0, 1.0)
    tau_theta  = pm.HalfStudentT("tau_theta", nu=3, sigma=1.0)

    theta_offset = pm.Normal("theta_offset", 0.0, 1.0, shape=J)
    theta        = pm.Deterministic("theta", mu_theta + tau_theta * theta_offset)

    intercept  = pm.Normal("intercept", 0.0, 5.0)
    beta_ctrl  = pm.Normal("beta_ctrl", 0.0, 1.0, shape=C.shape[1])

    sigma      = pm.HalfStudentT("sigma", nu=3, sigma=5.0)

    mu = intercept + pm.math.dot(X, theta) + pm.math.dot(C, beta_ctrl)
    y_obs = pm.Normal("y_obs", mu=mu, sigma=sigma, observed=y_c)

    # Sampling (<1000 draws per chain)
    trace_lmm = pm.sample(
        draws=400,      # < 1000 per chain
        tune=600,
        chains=2,
        target_accept=0.95,
        max_treedepth=12,
        random_seed=7,
        progressbar=True,
    )

# 3) Summary
display(az.summary(trace_lmm, var_names=["intercept","mu_theta","tau_theta","sigma","beta_ctrl","theta"], round_to=3))

# 4) Posterior → align shapes robustly
post = az.extract(trace_lmm)
theta_post = post["theta"].to_numpy()        # (draws, J_est)
J_est = theta_post.shape[1]

# Align in case upstream design changed
if J_est != J:
    print(f"[info] Posterior width {J_est} != design width {J}. Aligning by truncation.")
    J_final = min(J_est, J)
    theta_post = theta_post[:, :J_final]
    X_raw = X_raw[:, :J_final]
    ch_names_aligned = ch_names[:J_final]
else:
    J_final = J
    ch_names_aligned = ch_names

theta_mean = theta_post.mean(axis=0)

# Per-channel HDIs (works across ArviZ versions)
theta_hdi = np.empty((J_final, 2), dtype=float)
for j in range(J_final):
    h = np.asarray(az.hdi(theta_post[:, j], hdi_prob=0.95)).reshape(-1)
    theta_hdi[j, 0], theta_hdi[j, 1] = h[0], h[1]

# Channel table
ch_table = (
    pd.DataFrame({
        "channel": ch_names_aligned,
        "theta_mean": theta_mean,
        "theta_hdi_low": theta_hdi[:, 0],
        "theta_hdi_high": theta_hdi[:, 1],
    })
    .sort_values("theta_mean", ascending=False)
)
print("=== Random-slope (mixed) posterior for standardized channel multipliers ===")
display(ch_table)

# 5) Local “efficiency” near baseline (slope * avg adstocked spend, original units)
eff = []
for j, ch in enumerate(ch_names_aligned):
    eff.append({
        "channel": ch,
        "local_eff_mean": float(theta_mean[j] * X_raw[:, j].mean())
    })
eff = pd.DataFrame(eff).sort_values("local_eff_mean", ascending=False)
print("=== Local efficiency (slope * avg adstocked spend, original units) ===")
display(eff.head(12))

# 6) Posterior predictive (InferenceData) + clean PPC
with m_lmm:
    ppc_idata = pm.sample_posterior_predictive(
        trace_lmm,
        var_names=["y_obs"],
        return_inferencedata=True,
        random_seed=7,
    )

# Attach observed data only if it's not already present
if "observed_data" not in ppc_idata._groups_all:
    ppc_idata.add_groups({"observed_data": {"y_obs": y_c}})

az.plot_ppc(ppc_idata)

